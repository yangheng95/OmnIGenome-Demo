{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aca3fd9",
   "metadata": {},
   "source": [
    "# Genomic Foundation Model Auto-Benchmarking\n",
    "This script is used to auto-benchmark the Genomic Foundation Model on diversified downstream tasks. \n",
    "We have automated the benchmark pipeline based on the OmniGenome package. \n",
    "Once your foundation model is trained, you can use this script to evaluate the performance of the model. \n",
    "The script will automatically load the datasets, preprocess the data, and evaluate the model on the tasks. \n",
    "The script will output the performance of the model on each task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850472a",
   "metadata": {},
   "source": [
    "## [Optional] Prepare your own benchmark datasets\n",
    "We have provided a set of benchmark datasets in the tutorials, you can use them to evaluate the performance of the model.\n",
    "If you want to evaluate the model on your own datasets, you can prepare the datasets in the following steps:\n",
    "1. Prepare the datasets in the following format:\n",
    "    - The datasets should be in the `json` format.\n",
    "    - The datasets should contain two columns: `sequence` and `label`.\n",
    "    - The `sequence` column should contain the DNA sequences.\n",
    "    - The `label` column should contain the labels of the sequences.\n",
    "2. Save the datasets in a folder like the existing benchmark datasets. This folder is referred to as the `root` in the script.\n",
    "3. Place the model and tokenizer in an accessible folder.\n",
    "4. Sometimes the tokenizer does not work well with the datasets, you can write a custom tokenizer and model wrapper in the `omnigenome_wrapper.py` file.\n",
    "More detailed documentation on how to write the custom tokenizer and model wrapper will be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c8265",
   "metadata": {},
   "source": [
    "## Prepare the benchmark environment\n",
    "Before running the benchmark, you need to install the following required packages in addition to PyTorch and other dependencies.\n",
    "Find the installation instructions for PyTorch at https://pytorch.org/get-started/locally/.\n",
    "```bash\n",
    "pip install omnigenome, findfile, autocuda, metric-visualizer, transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b5309",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201378c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigenome import AutoBench\n",
    "import autocuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fecad8",
   "metadata": {},
   "source": [
    "## 1. Define the root folder of the benchmark datasets\n",
    "Define the root where the benchmark datasets are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d88847",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'RGB'  # Abbreviation of the RNA genome benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e50f4c",
   "metadata": {},
   "source": [
    "## 2. Define the model and tokenizer paths\n",
    "Provide the path to the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'anonymous8/OmniGenome-52M'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31d674",
   "metadata": {},
   "source": [
    "## 3. Initialize the AutoBench\n",
    "Select the available CUDA device based on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348811f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = autocuda.auto_cuda()\n",
    "auto_bench = AutoBench(\n",
    "    bench_root=root,\n",
    "    model_name_or_path=model_name_or_path,\n",
    "    device=device,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd396f5b",
   "metadata": {},
   "source": [
    "## 4. Run the benchmark\n",
    "The downstream tasks have predefined configurations for fair comparison.\n",
    "However, sometimes you might need to adjust the configuration based on your dataset or resources.\n",
    "For instance, adjusting the `max_length` or batch size.\n",
    "To adjust the configuration, you can override parameters in the `AutoBenchConfig` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 10\n",
    "seeds = [42, 43, 44]\n",
    "auto_bench.run(epochs=epochs, batch_size=batch_size, seeds=seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f3be7",
   "metadata": {},
   "source": [
    "## 5. Benchmark Checkpointing\n",
    "Whenever the benchmark is interrupted, the results will be saved and available for further execution.\n",
    "You can also clear the checkpoint to start fresh:\n",
    "```python\n",
    "AutoBench(bench_root=root, model_name_or_path=model_name_or_path, device=device, overwrite=True).run()\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
